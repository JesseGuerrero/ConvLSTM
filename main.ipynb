{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90dd120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953ba474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98ee874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovingMNISTDataset(Dataset):\n",
    "    \"\"\"Moving MNIST Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, root='./data', train=True, download=True):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        \n",
    "        if download:\n",
    "            self.download()\n",
    "        \n",
    "        # Load the same file for both train and test, but split differently\n",
    "        self.data = np.load(os.path.join(root, 'mnist_test_seq.npy'))\n",
    "        \n",
    "        # Split the data: use first 8000 sequences for train, rest for test\n",
    "        if train:\n",
    "            self.data = self.data[:, :8000, :, :]\n",
    "        else:\n",
    "            self.data = self.data[:, 8000:, :, :]\n",
    "    \n",
    "    def download(self):\n",
    "        os.makedirs(self.root, exist_ok=True)\n",
    "        url = 'http://www.cs.toronto.edu/~nitish/unsupervised_video/mnist_test_seq.npy'\n",
    "        filepath = os.path.join(self.root, 'mnist_test_seq.npy')\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            print('Downloading Moving MNIST dataset...')\n",
    "            urllib.request.urlretrieve(url, filepath)\n",
    "            print('Download completed!')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[1]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.data[:, idx, :, :].astype(np.float32) / 255.0\n",
    "        sequence = np.expand_dims(sequence, axis=1)  # Add channel dim\n",
    "        \n",
    "        input_seq = sequence[:10]   # First 10 frames\n",
    "        target_seq = sequence[10:]  # Last 10 frames\n",
    "        \n",
    "        return torch.tensor(input_seq), torch.tensor(target_seq)\n",
    "\n",
    "\n",
    "class ConvLSTMPredictor(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 input_dim=3,  # RGB channels\n",
    "                 temporal_dim=5,  # Temporal feature dimension\n",
    "                 hidden_dims=[64, 64, 64], \n",
    "                 kernel_size=(3, 3), \n",
    "                 num_layers=3,\n",
    "                 learning_rate=1e-3,\n",
    "                 batch_size=32,\n",
    "                 temporal_encoding='sinusoidal'):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Temporal encoder\n",
    "        self.temporal_encoder = TemporalEncoder(\n",
    "            encoding_type=temporal_encoding,\n",
    "            embed_dim=temporal_dim\n",
    "        )\n",
    "        \n",
    "        # Project temporal features to spatial dimensions\n",
    "        self.temporal_projection = nn.Linear(temporal_dim, input_dim)\n",
    "        \n",
    "        # ConvLSTM expects input_dim + temporal features\n",
    "        total_input_dim = input_dim + temporal_dim\n",
    "        \n",
    "        # Encoder ConvLSTM\n",
    "        self.encoder = ConvLSTM(\n",
    "            input_dim=total_input_dim,\n",
    "            hidden_dim=hidden_dims,\n",
    "            kernel_size=kernel_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bias=True,\n",
    "            return_all_layers=True\n",
    "        )\n",
    "        \n",
    "        # Decoder ConvLSTM  \n",
    "        self.decoder = ConvLSTM(\n",
    "            input_dim=total_input_dim,\n",
    "            hidden_dim=hidden_dims,\n",
    "            kernel_size=kernel_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bias=True,\n",
    "            return_all_layers=True\n",
    "        )\n",
    "        \n",
    "        # Output layer (back to RGB)\n",
    "        self.output_conv = nn.Conv2d(\n",
    "            in_channels=hidden_dims[-1],\n",
    "            out_channels=input_dim,\n",
    "            kernel_size=1\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.MSELoss()\n",
    "    \n",
    "    def _add_temporal_features(self, rgb_data, years, months, days):\n",
    "        \"\"\"Add temporal features to RGB data\"\"\"\n",
    "        batch_size, seq_len, channels, height, width = rgb_data.shape\n",
    "        \n",
    "        # Get temporal encodings\n",
    "        temporal_features = self.temporal_encoder(years, months, days)\n",
    "        # Shape: [batch, seq_len, temporal_dim]\n",
    "        \n",
    "        # Expand temporal features to spatial dimensions\n",
    "        temporal_spatial = temporal_features.unsqueeze(-1).unsqueeze(-1)\n",
    "        temporal_spatial = temporal_spatial.expand(-1, -1, -1, height, width)\n",
    "        # Shape: [batch, seq_len, temporal_dim, height, width]\n",
    "        \n",
    "        # Concatenate with RGB data\n",
    "        combined_data = torch.cat([rgb_data, temporal_spatial], dim=2)\n",
    "        # Shape: [batch, seq_len, channels + temporal_dim, height, width]\n",
    "        \n",
    "        return combined_data\n",
    "    \n",
    "    def forward(self, rgb_data, years, months, days, future_steps=10):\n",
    "        # Add temporal features to input\n",
    "        input_with_temporal = self._add_temporal_features(rgb_data, years, months, days)\n",
    "        \n",
    "        # Encode input sequence\n",
    "        _, encoder_states = self.encoder(input_with_temporal)\n",
    "        \n",
    "        # For prediction, you'll need future temporal information\n",
    "        # This is where you'd handle irregular temporal spacing\n",
    "        decoder_hidden = encoder_states\n",
    "        predictions = []\n",
    "        \n",
    "        # Use last input frame as initial decoder input\n",
    "        last_temporal = self._add_temporal_features(\n",
    "            rgb_data[:, -1:], years[:, -1:], months[:, -1:], days[:, -1:]\n",
    "        )\n",
    "        decoder_input = last_temporal\n",
    "        \n",
    "        for step in range(future_steps):\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "            \n",
    "            # Generate RGB prediction (remove temporal channels)\n",
    "            pred_frame = torch.sigmoid(self.output_conv(decoder_output[-1][:, -1]))\n",
    "            predictions.append(pred_frame.unsqueeze(1))\n",
    "            \n",
    "            # For next step, you'd need to predict/provide future temporal info\n",
    "            # This is application-specific\n",
    "            decoder_input = last_temporal  # Simplified\n",
    "        \n",
    "        return torch.cat(predictions, dim=1)\n",
    "\n",
    "\n",
    "def train_model(config=None):\n",
    "    \"\"\"Train the ConvLSTM model using PyTorch Lightning with wandb logging\"\"\"\n",
    "    \n",
    "    # Initialize wandb\n",
    "    wandb.init(\n",
    "        project=\"convlstm-moving-mnist\",\n",
    "        config=config or {\n",
    "            \"input_dim\": 1,\n",
    "            \"hidden_dims\": [64, 64, 64],\n",
    "            \"kernel_size\": [3, 3],  # Changed to list format for wandb compatibility\n",
    "            \"num_layers\": 3,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"batch_size\": 64,\n",
    "            \"max_epochs\": 50,\n",
    "            \"architecture\": \"ConvLSTM\",\n",
    "            \"dataset\": \"Moving MNIST\",\n",
    "            \"optimizer\": \"Adam\",\n",
    "            \"scheduler\": \"StepLR\"\n",
    "        },\n",
    "        tags=[\"convlstm\", \"video-prediction\", \"pytorch-lightning\"]\n",
    "    )\n",
    "    \n",
    "    # Initialize model with wandb config\n",
    "    model = ConvLSTMPredictor(\n",
    "        input_dim=wandb.config.input_dim,\n",
    "        hidden_dims=wandb.config.hidden_dims,\n",
    "        kernel_size=wandb.config.kernel_size,\n",
    "        num_layers=wandb.config.num_layers,\n",
    "        learning_rate=wandb.config.learning_rate,\n",
    "        batch_size=wandb.config.batch_size,\n",
    "        log_images=True,\n",
    "        log_frequency=100\n",
    "    )\n",
    "    \n",
    "    # Log model architecture\n",
    "    wandb.watch(model, log_freq=100, log_graph=True)\n",
    "    \n",
    "    # Callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val/loss',\n",
    "        dirpath='checkpoints/',\n",
    "        filename='convlstm-{epoch:02d}-{val_loss:.4f}',\n",
    "        save_top_k=3,\n",
    "        mode='min',\n",
    "        save_last=True\n",
    "    )\n",
    "    \n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "    \n",
    "    # Wandb Logger\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"convlstm-moving-mnist\",\n",
    "        log_model=\"all\",  # Log model checkpoints\n",
    "        save_dir=\"./wandb_logs\"\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=wandb.config.max_epochs,\n",
    "        accelerator='auto',\n",
    "        devices='auto',\n",
    "        callbacks=[checkpoint_callback, lr_monitor],\n",
    "        logger=wandb_logger,\n",
    "        log_every_n_steps=50,\n",
    "        val_check_interval=1.0,\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer.fit(model)\n",
    "    \n",
    "    # Test\n",
    "    trainer.test(model)\n",
    "    \n",
    "    # Log final metrics\n",
    "    wandb.log({\n",
    "        \"final_train_loss\": trainer.callback_metrics.get(\"train/loss_epoch\", 0),\n",
    "        \"final_val_loss\": trainer.callback_metrics.get(\"val/loss\", 0),\n",
    "        \"best_val_loss\": checkpoint_callback.best_model_score.item() if checkpoint_callback.best_model_score else 0\n",
    "    })\n",
    "    \n",
    "    # Finish wandb run\n",
    "    wandb.finish()\n",
    "    \n",
    "    return model, trainer\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    mode = 'train'  \n",
    "    print(\"Starting ConvLSTM training on Moving MNIST...\")\n",
    "    model, trainer = train_model()\n",
    "    print(\"Training completed!\")\n",
    "    print(\"View logs at: https://wandb.ai/\")\n",
    "    print(\"Best model saved in: checkpoints/\")\n",
    "        \n",
    "class TemporalEncoder(nn.Module):\n",
    "    def __init__(self, encoding_type='sinusoidal', embed_dim=32):\n",
    "        super().__init__()\n",
    "        self.encoding_type = encoding_type\n",
    "        if encoding_type == 'learned':\n",
    "            self.embeddings = nn.ModuleList([\n",
    "                nn.Embedding(10, embed_dim // 3),  # years\n",
    "                nn.Embedding(12, embed_dim // 3),  # months  \n",
    "                nn.Embedding(32, embed_dim // 3)   # days\n",
    "            ])\n",
    "        \n",
    "    def forward(self, years, months, days):\n",
    "        if self.encoding_type == 'sinusoidal':\n",
    "            year_norm = (years - 2020) / 4.0\n",
    "            month_rad = 2 * math.pi * (months - 1) / 12.0\n",
    "            day_rad = 2 * math.pi * days / 31.0\n",
    "            return torch.stack([year_norm, torch.sin(month_rad), torch.cos(month_rad), \n",
    "                              torch.sin(day_rad), torch.cos(day_rad)], dim=-1)\n",
    "        elif self.encoding_type == 'learned':\n",
    "            embs = [emb((vals - offset).long()) for emb, vals, offset in \n",
    "                   zip(self.embeddings, [years, months, days], [2020, 1, 0])]\n",
    "            return torch.cat(embs, dim=-1)\n",
    "        else:\n",
    "            return torch.stack([(years-2020)/4.0, (months-1)/11.0, days/31.0], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd00fa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Satellite ConvLSTM with Temporal Encoding\n",
      "============================================================\n",
      "Testing temporal encoding...\n",
      "Temporal encoding shape: torch.Size([3, 5])\n",
      "Sample encoding for 2023-01-15: tensor([ 0.6909,  0.0000,  1.0000,  0.2994, -0.9541])\n",
      "Sample encoding for 2023-06-20: tensor([ 0.6909,  0.5000, -0.8660, -0.6514, -0.7588])\n",
      "Sample encoding for 2024-12-25: tensor([ 0.7091, -0.5000,  0.8660, -0.9885,  0.1514])\n",
      "\n",
      "Testing dataset with temporal encoding...\n",
      "Found 48 .tif files, 44 sequences\n",
      "Input sequence shape: torch.Size([3, 3, 256, 256])\n",
      "Target sequence shape: torch.Size([2, 3, 256, 256])\n",
      "Input temporal shape: torch.Size([3, 3])\n",
      "Target temporal shape: torch.Size([2, 3])\n",
      "Input dates: tensor([[2.0200e+03, 1.0000e+00, 7.0000e+00],\n",
      "        [2.0200e+03, 1.0000e+00, 2.3000e+01],\n",
      "        [2.0200e+03, 4.0000e+00, 1.2000e+01]])\n",
      "Target dates: tensor([[2.0200e+03, 8.0000e+00, 2.0000e+00],\n",
      "        [2.0200e+03, 1.0000e+01, 5.0000e+00]])\n",
      "\n",
      "Analyzing temporal patterns...\n",
      "\n",
      "Seasonal temporal encodings:\n",
      "Winter: tensor([ 0.6909, -0.5000,  0.8660, -0.7908, -0.6121])\n",
      "Spring: tensor([ 0.6909,  0.8660,  0.5000, -0.7908, -0.6121])\n",
      "Summer: tensor([ 0.6909,  0.5000, -0.8660, -0.7908, -0.6121])\n",
      "Fall: tensor([ 0.6909, -0.8660, -0.5000, -0.7908, -0.6121])\n",
      "\n",
      "Year progression encodings (same date, different years):\n",
      "2020-06-15: 0.6364 (year norm), 0.5000 (month_sin)\n",
      "2021-06-15: 0.6545 (year norm), 0.5000 (month_sin)\n",
      "2022-06-15: 0.6727 (year norm), 0.5000 (month_sin)\n",
      "2023-06-15: 0.6909 (year norm), 0.5000 (month_sin)\n",
      "2024-06-15: 0.7091 (year norm), 0.5000 (month_sin)\n",
      "\n",
      "Starting Satellite ConvLSTM training with temporal encoding...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">effortless-violet-1</strong> at: <a href='https://wandb.ai/jesus-guerrero-ml/convlstm-satellite-temporal/runs/37pyz8w8' target=\"_blank\">https://wandb.ai/jesus-guerrero-ml/convlstm-satellite-temporal/runs/37pyz8w8</a><br> View project at: <a href='https://wandb.ai/jesus-guerrero-ml/convlstm-satellite-temporal' target=\"_blank\">https://wandb.ai/jesus-guerrero-ml/convlstm-satellite-temporal</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250611_141214-37pyz8w8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/projects/LSTM/wandb/run-20250611_141323-d0mzq3f9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jesus-guerrero-ml/convlstm-satellite-temporal/runs/d0mzq3f9' target=\"_blank\">apricot-cherry-2</a></strong> to <a href='https://wandb.ai/jesus-guerrero-ml/convlstm-satellite-temporal' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jesus-guerrero-ml/convlstm-satellite-temporal' target=\"_blank\">https://wandb.ai/jesus-guerrero-ml/convlstm-satellite-temporal</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jesus-guerrero-ml/convlstm-satellite-temporal/runs/d0mzq3f9' target=\"_blank\">https://wandb.ai/jesus-guerrero-ml/convlstm-satellite-temporal/runs/d0mzq3f9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/root/projects/LSTM/.venv/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/root/projects/LSTM/.venv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /root/projects/LSTM/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                    | Type                 | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | temporal_encoder        | TemporalEncoder      | 0      | train\n",
      "1 | encoder                 | ConvLSTM             | 557 K  | train\n",
      "2 | decoder                 | ConvLSTM             | 557 K  | train\n",
      "3 | encoder_temporal_fusion | TemporalFusionModule | 12.8 K | train\n",
      "4 | decoder_temporal_fusion | TemporalFusionModule | 12.8 K | train\n",
      "5 | output_conv             | Conv2d               | 195    | train\n",
      "6 | criterion               | MSELoss              | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.560     Total estimated model params size (MB)\n",
      "31        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48 .tif files, 34 sequences\n",
      "Splits - Train: 27, Val: 3, Test: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42af091cb004db4abf8a27cea51fd99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c25720ce664d64ba7972fb35fb335d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c98db893ba4a37915e42a6cc60b7b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd367efa8b8f4db092c5368c7474a1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb56e35140e44fd80dfcd238a3bfdea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff5fcdb3de7469ba5a43ca35f112a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3e813a79d742998bc5d9952f392fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3544f196778143b3a8d2bcea9ba74322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb51a0f3eb2c4083861fee6fa1206eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac90058ed8f4bdfb5c8f6a77319ffaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb3f76ce27a49dca7221c8beb2c2039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea6284b78bf4314a2e8e516f9713b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f46a2e143e4719bd19c9f99fde9e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047a5d3aa14648a6834ba10a3b316410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2da00d608a47df949da8aa5fb5f3db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3afbb4041241495eb6ce4f86a05c8d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fa0f5afe9d4f969873b078b9c66610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b94cb412a04b6eb870ee75735a7c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9158ca5ae3e4a65a0da355b47264d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8148c345034cc4b9d97ceab54133e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574a9e4a3b49411bb523a576019181b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a98e797bef04580a3bbaa405b5a3ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3387b775bd40f5a3725ca490da0ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc5205caedc4041aa78a1c7e1b8c209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128fba9385ea4024931ca49ee06b02be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc627b5bb89148b6b58acd46a885fcd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c93eca1729442bb7603b5e36da1293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39b7cc359ac4be389e0e188fee68d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f9d53f2e4f4f389276b6fea087cfc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9df9b7b1a6446a9eb47437294d910c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50c739987484ca889aa04088d48499a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90fc838e4b484690966853d93a43aeec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574b404266824248a8f683c377e30a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732ad0d04bc74b11b413e9ec49eb4627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3584636c5bbc4abca5a2b1ecac874f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538da2fc704d4e33a208d6b0e66bb132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7424250dc0904b2f868ccd382db6bdb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918c0790f4d64e78ba0de0ebb1ce5e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc89e3811094a3e96b09cc8fcbf7afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c0eb8a7c304077a868ea3e855b2171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34fbd66c06e4c3c959b434a90cfdd0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10bd45549be4af9a8f4e2dc3bf83bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bdaf095680443a19d7c931aa4a104c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19de802fbf148e4b5fba3c45c6b857e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0abd3b325f4841ac2c053510b77f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187dd6e51222403ba2f02964de66ced6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73b0a76b54a405a8be214616edfffbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaabb82584444a81869e712b7f9e3f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b303017574943c39b26051909541413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a431b6ec91c04028816f2ca43bb29172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9109112816e4d2c9350b4b9a41dc99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be8c0e9c7af4d30a038bc1feb1bcfbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419656f259f248aebfe213e444fadd75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b970ad88ec7c44a3b2118827ca03aeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353e2742ec5049a7b19b7821f76223de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9084a6ae72a341629312f8746af87284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82959223094471aa81225fc967e99f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b32b24627a84f5ebd84edb0e87a6d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c18c7fe8094793a41cd4c4b56a9d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3afdd4c9153c4611ae003f452fe72e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f5a80b3344480b90b505affd1045e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0608b660354779a6012e37db379e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbec875208b5404fb0e971acf1332f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4670c7d2ad432aae3c71daae82ce4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c93120e123b46a098c77d72db22523d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc9ffb246b44045a919c6b525fbbee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160936675c6244b981a1b271b6f9b0dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf9ec79bde14149a6f0e4f3a241ea07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413605d62a644164a8be7b009993422f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5be705fe9f47da9cbde5a895c412b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22831a1b640431baa82051dd53ace04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ad4fe9af5545d1bd9092580ad4d564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a2b11372ef49c0b46794a0abdb85d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae0771f2a20414caaa2a9d1b13cdfd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cb726b6b6d4d4dafcbcef7ea5aa3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856a17c842b8493bb61d678fe1df16b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216d989a695c4f40a413e75f1fa79c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb8adf40fd342b8844f11fdeb9e07b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b36cb162744eed82158f9087b8e808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f715ad288804c209e45abdb4802d75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a93464c59a24da6b58ef1f942961df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55bb09debc646208a58cf3106e6cc43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feaf967c7c694da49ea9983a5989d968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c5ae3490b34f4aaf934758aa4282e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16dcdd03230d4d799ff5d820375ed527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af7bbfdcacc47d79ce6612f9ebc16a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9febe1fe924949dca762de1acb05ec66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524d0e0c0c3d4ea2b83a7fe33ee2ffc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e5c88bdf154a2dafaa699dc88ea446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692c59e0e07045c6b0d099da520cc2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76bb699ceee4f9580cf9c326e3f0a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ee0d75be354ee5a1006b063509bdf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73c654085294cc4bbf494d6dd408986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d824d2d5144c0c97e4d42b62bbe6a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6913b88a692048328267f08675882492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d13213e9f5543c1936900f9ea1004a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460cde57cd304cec9d5bbe5ac1c09dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8851a48f14a343039aee1238941064a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18a91a4e7564bbbb1996de53c263f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6583b0bfab8d47fdb65db2028b7074c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c886c373dd1c4855877850d7ca541400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31652fd7d6b4d569de28364d94baebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48 .tif files, 34 sequences\n",
      "Splits - Train: 27, Val: 3, Test: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5b893156344c38ba19089aa5db421d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "     test/loss_epoch       0.0025221684481948614\n",
      "        test/mae           0.033692747354507446\n",
      "        test/mse           0.0025221684481948614\n",
      "        test/psnr           25.982460021972656\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Data row contained incompatible types:\n{'Metric': 'temporal_encoding_enabled', 'Value': True} of type {'Metric': String, 'Value': Boolean} is not assignable to {'Metric': None or String, 'Value': None or Number}\nKey 'Value':\n\tBoolean not assignable to None or Number\n\t\tBoolean not assignable to None\n\tand\n\t\tBoolean not assignable to Number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 758\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Satellite ConvLSTM training with temporal encoding...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 758\u001b[0m model, trainer \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_satellite_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mView logs at: https://wandb.ai/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 644\u001b[0m, in \u001b[0;36mtrain_satellite_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m final_metrics\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    642\u001b[0m     summary_data\u001b[38;5;241m.\u001b[39mappend([key, value])\n\u001b[0;32m--> 644\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msummary_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMetric\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mValue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_metrics_table\u001b[39m\u001b[38;5;124m\"\u001b[39m: table})\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# Finish wandb run\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/LSTM/.venv/lib/python3.10/site-packages/wandb/sdk/data_types/table.py:284\u001b[0m, in \u001b[0;36mTable.__init__\u001b[0;34m(self, columns, data, rows, dataframe, dtype, optional, allow_mixed_types, log_mode)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_from_dataframe(data, columns, optional, dtype)\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_from_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# legacy\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m rows \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/LSTM/.venv/lib/python3.10/site-packages/wandb/sdk/data_types/table.py:314\u001b[0m, in \u001b[0;36mTable._init_from_list\u001b[0;34m(self, data, columns, optional, dtype)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_column_types(dtype, optional)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/LSTM/.venv/lib/python3.10/site-packages/wandb/sdk/data_types/table_decorators.py:42\u001b[0m, in \u001b[0;36mallow_relogging_after_mutation.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIMMUTABLE\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_been_logged:\n\u001b[1;32m     35\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mtermwarn(\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are mutating a Table with log_mode=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIMMUTABLE\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m that has been \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogged already. Subsequent log() calls will have no effect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSet log_mode=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMUTABLE\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to enable re-logging after mutations\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     39\u001b[0m         repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m     )\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/LSTM/.venv/lib/python3.10/site-packages/wandb/sdk/data_types/table_decorators.py:58\u001b[0m, in \u001b[0;36mallow_incremental_logging_after_append.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(method)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m: wandb\u001b[38;5;241m.\u001b[39mTable, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[0;32m---> 58\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINCREMENTAL\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_artifact_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/projects/LSTM/.venv/lib/python3.10/site-packages/wandb/sdk/data_types/table.py:529\u001b[0m, in \u001b[0;36mTable.add_data\u001b[0;34m(self, *data)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcast(\n\u001b[1;32m    523\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns[ndx],\n\u001b[1;32m    524\u001b[0m             _dtypes\u001b[38;5;241m.\u001b[39mTypeRegistry\u001b[38;5;241m.\u001b[39mtype_of(item),\n\u001b[1;32m    525\u001b[0m             optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    526\u001b[0m         )\n\u001b[1;32m    528\u001b[0m \u001b[38;5;66;03m# Update the table's column types\u001b[39;00m\n\u001b[0;32m--> 529\u001b[0m result_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_updated_result_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_types \u001b[38;5;241m=\u001b[39m result_type\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# rows need to be mutable\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/LSTM/.venv/lib/python3.10/site-packages/wandb/sdk/data_types/table.py:553\u001b[0m, in \u001b[0;36mTable._get_updated_result_type\u001b[0;34m(self, row)\u001b[0m\n\u001b[1;32m    551\u001b[0m result_type \u001b[38;5;241m=\u001b[39m current_type\u001b[38;5;241m.\u001b[39massign(incoming_row_dict)\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result_type, _dtypes\u001b[38;5;241m.\u001b[39mInvalidType):\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    554\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData row contained incompatible types:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcurrent_type\u001b[38;5;241m.\u001b[39mexplain(incoming_row_dict)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    555\u001b[0m     )\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_type\n",
      "\u001b[0;31mTypeError\u001b[0m: Data row contained incompatible types:\n{'Metric': 'temporal_encoding_enabled', 'Value': True} of type {'Metric': String, 'Value': Boolean} is not assignable to {'Metric': None or String, 'Value': None or Number}\nKey 'Value':\n\tBoolean not assignable to None or Number\n\t\tBoolean not assignable to None\n\tand\n\t\tBoolean not assignable to Number"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import torchvision.transforms as transforms\n",
    "from datetime import datetime\n",
    "import re\n",
    "from typing import List, Tuple, Optional\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import wandb\n",
    "import math\n",
    "\n",
    "class TemporalEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=5):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "    \n",
    "    def forward(self, years, months, days):\n",
    "        return self._sinusoidal_encoding(years, months, days)\n",
    "    \n",
    "    def _sinusoidal_encoding(self, years, months, days):        \n",
    "        # Year encoding (linear trend)\n",
    "        year_norm = (years - 1985) / 55.0\n",
    "        \n",
    "        # Month encoding (cyclical)\n",
    "        month_rad = 2 * math.pi * (months - 1) / 12.0\n",
    "        month_sin = torch.sin(month_rad)\n",
    "        month_cos = torch.cos(month_rad)\n",
    "        \n",
    "        # Day encoding (cyclical within month)\n",
    "        day_rad = 2 * math.pi * (days - 1) / 31.0\n",
    "        day_sin = torch.sin(day_rad)\n",
    "        day_cos = torch.cos(day_rad)\n",
    "        \n",
    "        # Combine features [batch_size, 5]\n",
    "        temporal_features = torch.stack([\n",
    "            year_norm, month_sin, month_cos, \n",
    "            day_sin, day_cos\n",
    "        ], dim=-1)\n",
    "        \n",
    "        return temporal_features\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    \"\"\"ConvLSTM cell implementation.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        \n",
    "        self.conv = nn.Conv2d(input_dim + hidden_dim, 4 * hidden_dim, \n",
    "                             kernel_size, padding=self.padding, bias=bias)\n",
    "    \n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        \n",
    "        i, f, o = torch.sigmoid(cc_i), torch.sigmoid(cc_f), torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "        \n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "    \n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        h, w = image_size\n",
    "        device = self.conv.weight.device\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, h, w, device=device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, h, w, device=device))\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    \"\"\"Multi-layer ConvLSTM.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers, batch_first=True, bias=True):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim if isinstance(hidden_dim, list) else [hidden_dim] * num_layers\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, list) else [kernel_size] * num_layers\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "        self.cell_list = nn.ModuleList([\n",
    "            ConvLSTMCell(input_dim if i == 0 else self.hidden_dim[i-1], \n",
    "                        self.hidden_dim[i], self.kernel_size[i], bias)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        if not self.batch_first:\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "        \n",
    "        b, seq_len, _, h, w = input_tensor.size()\n",
    "        \n",
    "        if hidden_state is None:\n",
    "            hidden_state = [cell.init_hidden(b, (h, w)) for cell in self.cell_list]\n",
    "        \n",
    "        layer_output_list, last_state_list = [], []\n",
    "        cur_layer_input = input_tensor\n",
    "        \n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            \n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](cur_layer_input[:, t], [h, c])\n",
    "                output_inner.append(h)\n",
    "            \n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "        \n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "class TemporalFusionModule(nn.Module):\n",
    "    \"\"\"Module to fuse temporal encoding with spatial features.\"\"\"\n",
    "    def __init__(self, temporal_dim=5, spatial_channels=64, fused_channels=64):\n",
    "        super().__init__()\n",
    "        self.temporal_dim = temporal_dim\n",
    "        self.spatial_channels = spatial_channels\n",
    "        self.fused_channels = fused_channels\n",
    "        \n",
    "        # Project temporal features to spatial dimensions\n",
    "        self.temporal_proj = nn.Sequential(\n",
    "            nn.Linear(temporal_dim, spatial_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(spatial_channels, spatial_channels)\n",
    "        )\n",
    "        \n",
    "        # Fusion layer\n",
    "        self.fusion_conv = nn.Conv2d(spatial_channels * 2, fused_channels, 1)\n",
    "        \n",
    "    def forward(self, spatial_features, temporal_features):\n",
    "        # spatial_features: [batch, channels, height, width]\n",
    "        # temporal_features: [batch, temporal_dim]\n",
    "        \n",
    "        batch_size, channels, height, width = spatial_features.shape\n",
    "        \n",
    "        # Project temporal features\n",
    "        temporal_proj = self.temporal_proj(temporal_features)  # [batch, spatial_channels]\n",
    "        \n",
    "        # Expand temporal features to spatial dimensions\n",
    "        temporal_spatial = temporal_proj.unsqueeze(-1).unsqueeze(-1).expand(\n",
    "            batch_size, self.spatial_channels, height, width\n",
    "        )\n",
    "        \n",
    "        # Concatenate and fuse\n",
    "        combined = torch.cat([spatial_features, temporal_spatial], dim=1)\n",
    "        fused = self.fusion_conv(combined)\n",
    "        \n",
    "        return fused\n",
    "\n",
    "class SanAntonioSatelliteDataset(Dataset):\n",
    "    \"\"\"Enhanced dataset with temporal encoding support.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: str, sequence_length: int = 5, target_length: int = 3,\n",
    "                 image_size: int = 512, normalize: bool = True, temporal_stride: int = 1):\n",
    "        self.data_dir = data_dir\n",
    "        self.sequence_length = sequence_length\n",
    "        self.target_length = target_length\n",
    "        self.image_size = image_size\n",
    "        self.normalize = normalize\n",
    "        self.temporal_stride = temporal_stride\n",
    "        \n",
    "        # Get sorted .tif files by date\n",
    "        self.tif_files = sorted(\n",
    "            glob.glob(os.path.join(data_dir, \"*.tif\")),\n",
    "            key=lambda f: datetime.strptime(re.search(r'(\\d{4}-\\d{2}-\\d{2})', f).group(1), '%Y-%m-%d')\n",
    "        )\n",
    "        \n",
    "        # Valid sequence starting indices\n",
    "        total_needed = (sequence_length + target_length - 1) * temporal_stride + 1\n",
    "        self.valid_sequences = list(range(len(self.tif_files) - total_needed + 1))\n",
    "        \n",
    "        print(f\"Found {len(self.tif_files)} .tif files, {len(self.valid_sequences)} sequences\")\n",
    "    \n",
    "    def _extract_date_from_filename(self, filename: str) -> Tuple[int, int, int]:\n",
    "        \"\"\"Extract year, month, day from filename.\"\"\"\n",
    "        # Extract date from San_Antonio_YYYY-MM-DD.tif format\n",
    "        date_match = re.search(r'(\\d{4})-(\\d{2})-(\\d{2})', filename)\n",
    "        if date_match:\n",
    "            year, month, day = map(int, date_match.groups())\n",
    "            return year, month, day\n",
    "        else:\n",
    "            raise ValueError(f\"Could not extract date from filename: {filename}\")\n",
    "    \n",
    "    def _load_and_crop_tif(self, tif_path: str) -> torch.Tensor:\n",
    "        \"\"\"Load RGB channels from .tif and center crop to target size.\"\"\"\n",
    "        # Load with rioxarray (automatically handles CRS, transforms, etc.)\n",
    "        da = rxr.open_rasterio(tif_path, chunks={'band': 1, 'x': 512, 'y': 512})\n",
    "        \n",
    "        # Take first 3 bands as RGB, center crop\n",
    "        rgb = da.isel(band=slice(0, 3))\n",
    "        h, w = rgb.sizes['y'], rgb.sizes['x']\n",
    "        \n",
    "        # Center crop indices\n",
    "        center_y, center_x = h // 2, w // 2\n",
    "        half_size = self.image_size // 2\n",
    "        y_slice = slice(max(0, center_y - half_size), center_y + half_size)\n",
    "        x_slice = slice(max(0, center_x - half_size), center_x + half_size)\n",
    "        \n",
    "        cropped = rgb.isel(y=y_slice, x=x_slice)\n",
    "        \n",
    "        # Convert to numpy and ensure correct shape/dtype\n",
    "        data = cropped.values.astype(np.float32)\n",
    "        \n",
    "        # Pad if necessary to reach target size\n",
    "        if data.shape[1] < self.image_size or data.shape[2] < self.image_size:\n",
    "            padded = np.zeros((3, self.image_size, self.image_size), dtype=np.float32)\n",
    "            h, w = data.shape[1], data.shape[2]\n",
    "            start_h, start_w = (self.image_size - h) // 2, (self.image_size - w) // 2\n",
    "            padded[:, start_h:start_h+h, start_w:start_w+w] = data\n",
    "            data = padded\n",
    "        \n",
    "        # Normalize to [0,1]\n",
    "        if self.normalize:\n",
    "            if data.max() > 1:  # Assume uint8/uint16 if values > 1\n",
    "                data = data / (65535.0 if data.max() > 255 else 255.0)\n",
    "            else:\n",
    "                # Percentile normalization for float data\n",
    "                p1, p99 = np.percentile(data, [1, 99])\n",
    "                if p99 > p1:\n",
    "                    data = np.clip((data - p1) / (p99 - p1), 0, 1)\n",
    "        \n",
    "        return torch.from_numpy(data)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.valid_sequences)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        start_idx = self.valid_sequences[idx]\n",
    "        \n",
    "        # Get file indices for input and target sequences\n",
    "        input_indices = [start_idx + i * self.temporal_stride for i in range(self.sequence_length)]\n",
    "        target_indices = [start_idx + (self.sequence_length + i) * self.temporal_stride \n",
    "                         for i in range(self.target_length)]\n",
    "        \n",
    "        # Load image sequences\n",
    "        input_seq = torch.stack([self._load_and_crop_tif(self.tif_files[i]) for i in input_indices])\n",
    "        target_seq = torch.stack([self._load_and_crop_tif(self.tif_files[i]) for i in target_indices])\n",
    "        \n",
    "        # Extract temporal information\n",
    "        input_dates = []\n",
    "        target_dates = []\n",
    "        \n",
    "        for i in input_indices:\n",
    "            year, month, day = self._extract_date_from_filename(self.tif_files[i])\n",
    "            input_dates.append([year, month, day])\n",
    "            \n",
    "        for i in target_indices:\n",
    "            year, month, day = self._extract_date_from_filename(self.tif_files[i])\n",
    "            target_dates.append([year, month, day])\n",
    "        \n",
    "        input_temporal = torch.tensor(input_dates, dtype=torch.float32)  # [seq_len, 3]\n",
    "        target_temporal = torch.tensor(target_dates, dtype=torch.float32)  # [target_len, 3]\n",
    "        \n",
    "        return input_seq, target_seq, input_temporal, target_temporal\n",
    "\n",
    "class SanAntonioDataModule(pl.LightningDataModule):\n",
    "    \"\"\"Lightning DataModule for San Antonio satellite data with temporal encoding.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: str, sequence_length: int = 10, target_length: int = 5,\n",
    "                 image_size: int = 512, batch_size: int = 4, num_workers: int = 4,\n",
    "                 train_split: float = 0.8, val_split: float = 0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        dataset = SanAntonioSatelliteDataset(\n",
    "            self.hparams.data_dir, \n",
    "            self.hparams.sequence_length, \n",
    "            self.hparams.target_length,\n",
    "            self.hparams.image_size, \n",
    "            **{k: v for k, v in self.hparams.items() \n",
    "               if k not in ['data_dir', 'batch_size', 'num_workers', 'train_split', 'val_split', \n",
    "                           'sequence_length', 'target_length', 'image_size']}\n",
    "        )\n",
    "        \n",
    "        # Split dataset\n",
    "        total = len(dataset)\n",
    "        train_size = int(self.hparams.train_split * total)\n",
    "        val_size = int(self.hparams.val_split * total)\n",
    "        \n",
    "        self.train_dataset = torch.utils.data.Subset(dataset, range(train_size))\n",
    "        self.val_dataset = torch.utils.data.Subset(dataset, range(train_size, train_size + val_size))\n",
    "        self.test_dataset = torch.utils.data.Subset(dataset, range(train_size + val_size, total))\n",
    "        \n",
    "        print(f\"Splits - Train: {len(self.train_dataset)}, Val: {len(self.val_dataset)}, Test: {len(self.test_dataset)}\")\n",
    "    \n",
    "    def _dataloader(self, dataset, shuffle=False):\n",
    "        return DataLoader(dataset, batch_size=self.hparams.batch_size, shuffle=shuffle,\n",
    "                         num_workers=self.hparams.num_workers, pin_memory=True,\n",
    "                         persistent_workers=self.hparams.num_workers > 0)\n",
    "    \n",
    "    def train_dataloader(self): return self._dataloader(self.train_dataset, shuffle=True)\n",
    "    def val_dataloader(self): return self._dataloader(self.val_dataset)\n",
    "    def test_dataloader(self): return self._dataloader(self.test_dataset)\n",
    "\n",
    "class SatelliteConvLSTMPredictor(pl.LightningModule):\n",
    "    \"\"\"Enhanced ConvLSTM model with temporal encoding for satellite imagery prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=3, hidden_dims=[64, 64, 64], kernel_size=(3, 3), \n",
    "                 num_layers=3, learning_rate=1e-3, target_length=5, batch_size=4,\n",
    "                 temporal_dim=5, use_temporal_fusion=True,\n",
    "                 log_images=True, log_frequency=100):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Temporal encoder\n",
    "        self.temporal_encoder = TemporalEncoder(embed_dim=temporal_dim)\n",
    "        \n",
    "        # Both encoder and decoder use same input dimensions\n",
    "        self.encoder = ConvLSTM(input_dim, hidden_dims, kernel_size, num_layers, True, True)\n",
    "        self.decoder = ConvLSTM(input_dim, hidden_dims, kernel_size, num_layers, True, True)\n",
    "        \n",
    "        # Temporal fusion modules\n",
    "        if use_temporal_fusion:\n",
    "            self.encoder_temporal_fusion = TemporalFusionModule(\n",
    "                temporal_dim, hidden_dims[-1], hidden_dims[-1]\n",
    "            )\n",
    "            self.decoder_temporal_fusion = TemporalFusionModule(\n",
    "                temporal_dim, hidden_dims[-1], hidden_dims[-1]\n",
    "            )\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_conv = nn.Conv2d(hidden_dims[-1], input_dim, 1)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        # For logging\n",
    "        self.log_images = log_images\n",
    "        self.log_frequency = log_frequency\n",
    "        self.step_count = 0\n",
    "    \n",
    "    def forward(self, x, input_temporal, target_temporal):\n",
    "        # x: [batch, seq_len, channels, height, width]\n",
    "        # input_temporal: [batch, seq_len, 3] (year, month, day)\n",
    "        # target_temporal: [batch, target_len, 3]\n",
    "        \n",
    "        batch_size, seq_len = x.shape[:2]\n",
    "        target_len = target_temporal.shape[1]\n",
    "        \n",
    "        # Encode input sequence\n",
    "        encoder_outputs, encoder_states = self.encoder(x)\n",
    "        \n",
    "        # Apply temporal fusion to encoder states if enabled\n",
    "        if self.hparams.use_temporal_fusion:\n",
    "            # Use last input temporal encoding for encoder fusion\n",
    "            last_input_temporal = input_temporal[:, -1]  # [batch, 3]\n",
    "            temporal_encoding = self.temporal_encoder(\n",
    "                last_input_temporal[:, 0], \n",
    "                last_input_temporal[:, 1], \n",
    "                last_input_temporal[:, 2]\n",
    "            )\n",
    "            \n",
    "            # Fuse with encoder output\n",
    "            encoder_features = encoder_outputs[-1][:, -1]  # [batch, hidden_dim, H, W]\n",
    "            fused_encoder = self.encoder_temporal_fusion(encoder_features, temporal_encoding)\n",
    "            \n",
    "            # Update encoder states\n",
    "            encoder_states[-1][0] = fused_encoder\n",
    "        \n",
    "        # Decode target sequence\n",
    "        predictions = []\n",
    "        decoder_input = x[:, -1:, :, :, :]  # Start with last input frame\n",
    "        decoder_hidden = encoder_states\n",
    "        \n",
    "        for t in range(target_len):\n",
    "            # Get temporal encoding for current target timestep\n",
    "            current_temporal = target_temporal[:, t]  # [batch, 3]\n",
    "            temporal_encoding = self.temporal_encoder(\n",
    "                current_temporal[:, 0], \n",
    "                current_temporal[:, 1], \n",
    "                current_temporal[:, 2]\n",
    "            )\n",
    "            \n",
    "            # Decode one step\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "            \n",
    "            # Apply temporal fusion if enabled\n",
    "            if self.hparams.use_temporal_fusion:\n",
    "                decoder_features = decoder_output[-1][:, -1]  # [batch, hidden_dim, H, W]\n",
    "                fused_decoder = self.decoder_temporal_fusion(decoder_features, temporal_encoding)\n",
    "            else:\n",
    "                fused_decoder = decoder_output[-1][:, -1]\n",
    "            \n",
    "            # Generate prediction\n",
    "            pred_frame = torch.sigmoid(self.output_conv(fused_decoder))\n",
    "            predictions.append(pred_frame.unsqueeze(1))\n",
    "            \n",
    "            # Use prediction as next input\n",
    "            decoder_input = pred_frame.unsqueeze(1)\n",
    "        \n",
    "        return torch.cat(predictions, dim=1)\n",
    "    \n",
    "    def _step(self, batch, stage):\n",
    "        input_seq, target_seq, input_temporal, target_temporal = batch\n",
    "        predictions = self(input_seq.float(), input_temporal, target_temporal)\n",
    "        loss = self.criterion(predictions, target_seq.float())\n",
    "        \n",
    "        # Enhanced logging with additional metrics\n",
    "        self.log(f'{stage}/loss', loss, prog_bar=True, sync_dist=True, on_epoch=True, on_step=True)\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        if stage in ['val', 'test']:\n",
    "            with torch.no_grad():\n",
    "                mae = torch.mean(torch.abs(predictions - target_seq.float()))\n",
    "                mse = torch.mean((predictions - target_seq.float()) ** 2)\n",
    "                psnr = 20 * torch.log10(1.0 / torch.sqrt(mse + 1e-8))\n",
    "                \n",
    "                self.log(f'{stage}/mae', mae, sync_dist=True)\n",
    "                self.log(f'{stage}/mse', mse, sync_dist=True)\n",
    "                self.log(f'{stage}/psnr', psnr, sync_dist=True)\n",
    "        \n",
    "        # Log images periodically\n",
    "        if self.log_images and self.step_count % self.log_frequency == 0 and stage == 'val':\n",
    "            self._log_prediction_images(input_seq, target_seq, predictions, input_temporal, target_temporal, stage)\n",
    "        \n",
    "        self.step_count += 1\n",
    "        return loss\n",
    "    \n",
    "    def _log_prediction_images(self, input_seq, target_seq, predictions, input_temporal, target_temporal, stage):\n",
    "        \"\"\"Log prediction visualizations with temporal information to wandb\"\"\"\n",
    "        try:\n",
    "            # Take first sample from batch\n",
    "            input_sample = input_seq[0].detach().cpu().float().numpy()\n",
    "            target_sample = target_seq[0].detach().cpu().float().numpy()\n",
    "            pred_sample = predictions[0].detach().cpu().float().numpy()\n",
    "            input_temp = input_temporal[0].detach().cpu().numpy()\n",
    "            target_temp = target_temporal[0].detach().cpu().numpy()\n",
    "            \n",
    "            # Create visualization\n",
    "            num_frames = min(5, self.hparams.target_length)\n",
    "            fig, axes = plt.subplots(3, num_frames, figsize=(num_frames * 4, 12))\n",
    "            \n",
    "            if num_frames == 1:\n",
    "                axes = axes.reshape(3, 1)\n",
    "            \n",
    "            for t in range(num_frames):\n",
    "                # Last input frame (only show in first column)\n",
    "                if t == 0:\n",
    "                    input_img = np.transpose(input_sample[-1], (1, 2, 0)).astype(np.float32)\n",
    "                    input_img = np.clip(input_img, 0, 1)\n",
    "                    axes[0, t].imshow(input_img)\n",
    "                    # Add temporal info\n",
    "                    last_date = input_temp[-1]\n",
    "                    axes[0, t].set_title(f'Last Input\\n{int(last_date[0])}-{int(last_date[1]):02d}-{int(last_date[2]):02d}', \n",
    "                                        fontsize=10)\n",
    "                else:\n",
    "                    axes[0, t].axis('off')\n",
    "                \n",
    "                # Target frame\n",
    "                target_img = np.transpose(target_sample[t], (1, 2, 0)).astype(np.float32)\n",
    "                target_img = np.clip(target_img, 0, 1)\n",
    "                axes[1, t].imshow(target_img)\n",
    "                target_date = target_temp[t]\n",
    "                axes[1, t].set_title(f'Target {t+1}\\n{int(target_date[0])}-{int(target_date[1]):02d}-{int(target_date[2]):02d}', \n",
    "                                    fontsize=10)\n",
    "                \n",
    "                # Predicted frame\n",
    "                pred_img = np.transpose(pred_sample[t], (1, 2, 0)).astype(np.float32)\n",
    "                pred_img = np.clip(pred_img, 0, 1)\n",
    "                axes[2, t].imshow(pred_img)\n",
    "                axes[2, t].set_title(f'Predicted {t+1}\\n{int(target_date[0])}-{int(target_date[1]):02d}-{int(target_date[2]):02d}', \n",
    "                                    fontsize=10)\n",
    "                \n",
    "                # Remove axes\n",
    "                for i in range(3):\n",
    "                    axes[i, t].set_xticks([])\n",
    "                    axes[i, t].set_yticks([])\n",
    "            \n",
    "            # Add row labels\n",
    "            axes[0, 0].set_ylabel('Input', fontsize=14, rotation=90, labelpad=20)\n",
    "            axes[1, 0].set_ylabel('Target', fontsize=14, rotation=90, labelpad=20)\n",
    "            axes[2, 0].set_ylabel('Predicted', fontsize=14, rotation=90, labelpad=20)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Log to wandb\n",
    "            if hasattr(self.logger, 'experiment'):\n",
    "                self.logger.experiment.log({\n",
    "                    f'{stage}/predictions_with_dates': wandb.Image(fig),\n",
    "                    'epoch': self.current_epoch,\n",
    "                    'step': self.step_count\n",
    "                })\n",
    "            \n",
    "            plt.close(fig)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error logging images: {e}\")\n",
    "            import traceback\n",
    "            print(f\"Full traceback: {traceback.format_exc()}\")\n",
    "    \n",
    "    def training_step(self, batch, batch_idx): \n",
    "        return self._step(batch, 'train')\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx): \n",
    "        return self._step(batch, 'val')\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._step(batch, 'test')\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "def train_satellite_model(config=None):\n",
    "    \"\"\"Train the Satellite ConvLSTM model with temporal encoding using PyTorch Lightning\"\"\"\n",
    "    \n",
    "    # Initialize wandb\n",
    "    wandb.init(\n",
    "        project=\"convlstm-satellite-temporal\",\n",
    "        config=config or {\n",
    "            \"input_dim\": 3,\n",
    "            \"hidden_dims\": [32, 64, 64],\n",
    "            \"kernel_size\": [3, 3],\n",
    "            \"num_layers\": 3,\n",
    "            \"learning_rate\": 1e-4,\n",
    "            \"batch_size\": 2,\n",
    "            \"max_epochs\": 50,\n",
    "            \"sequence_length\": 10,\n",
    "            \"target_length\": 5,\n",
    "            \"image_size\": 512,\n",
    "            \"temporal_dim\": 5,\n",
    "            \"use_temporal_fusion\": True,\n",
    "            \"architecture\": \"ConvLSTM-Temporal\",\n",
    "            \"dataset\": \"San Antonio Satellite\",\n",
    "            \"optimizer\": \"Adam\",\n",
    "            \"scheduler\": \"StepLR\",\n",
    "            \"precision\": \"16-mixed\",\n",
    "            \"gradient_clip_val\": 1.0,\n",
    "            \"data_dir\": \"./San_Antonio\",\n",
    "            \"train_split\": 0.8,\n",
    "            \"val_split\": 0.1,\n",
    "            \"num_workers\": 2,\n",
    "            \"log_images\": True,\n",
    "            \"log_frequency\": 50\n",
    "        },\n",
    "        tags=[\"convlstm\", \"temporal-encoding\", \"satellite-prediction\", \"pytorch-lightning\", \"san-antonio\"]\n",
    "    )\n",
    "    \n",
    "    # Data module\n",
    "    data_module = SanAntonioDataModule(\n",
    "        data_dir=wandb.config.data_dir,\n",
    "        sequence_length=wandb.config.sequence_length,\n",
    "        target_length=wandb.config.target_length,\n",
    "        image_size=wandb.config.image_size,\n",
    "        batch_size=wandb.config.batch_size,\n",
    "        num_workers=wandb.config.num_workers,\n",
    "        train_split=wandb.config.train_split,\n",
    "        val_split=wandb.config.val_split\n",
    "    )\n",
    "    \n",
    "    # Initialize model with wandb config\n",
    "    model = SatelliteConvLSTMPredictor(\n",
    "        input_dim=wandb.config.input_dim,\n",
    "        hidden_dims=wandb.config.hidden_dims,\n",
    "        kernel_size=tuple(wandb.config.kernel_size),\n",
    "        num_layers=wandb.config.num_layers,\n",
    "        learning_rate=wandb.config.learning_rate,\n",
    "        target_length=wandb.config.target_length,\n",
    "        batch_size=wandb.config.batch_size,\n",
    "        temporal_dim=wandb.config.temporal_dim,\n",
    "        use_temporal_fusion=wandb.config.use_temporal_fusion,\n",
    "        log_images=wandb.config.log_images,\n",
    "        log_frequency=wandb.config.log_frequency\n",
    "    )\n",
    "    \n",
    "    # Log model architecture\n",
    "    wandb.watch(model, log_freq=100, log_graph=True)\n",
    "    \n",
    "    # Callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val/loss',\n",
    "        dirpath='checkpoints/',\n",
    "        filename='satellite-convlstm-temporal-{epoch:02d}-{val_loss:.4f}',\n",
    "        save_top_k=3,\n",
    "        mode='min',\n",
    "        save_last=True\n",
    "    )\n",
    "    \n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "    \n",
    "    # Wandb Logger\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"convlstm-satellite-temporal\",\n",
    "        log_model=\"all\",\n",
    "        save_dir=\"./wandb_logs\"\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=wandb.config.max_epochs,\n",
    "        accelerator='auto',\n",
    "        devices=1,\n",
    "        precision=wandb.config.precision,\n",
    "        gradient_clip_val=wandb.config.gradient_clip_val,\n",
    "        callbacks=[checkpoint_callback, lr_monitor],\n",
    "        logger=wandb_logger,\n",
    "        log_every_n_steps=10,\n",
    "        val_check_interval=0.5,\n",
    "        limit_val_batches=10,\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer.fit(model, data_module)\n",
    "    \n",
    "    # Test\n",
    "    trainer.test(model, data_module)\n",
    "    \n",
    "    # Log final metrics\n",
    "    final_metrics = {\n",
    "        \"final_train_loss\": trainer.callback_metrics.get(\"train/loss_epoch\", 0),\n",
    "        \"final_val_loss\": trainer.callback_metrics.get(\"val/loss\", 0),\n",
    "        \"final_val_mae\": trainer.callback_metrics.get(\"val/mae\", 0),\n",
    "        \"final_val_psnr\": trainer.callback_metrics.get(\"val/psnr\", 0),\n",
    "        \"best_val_loss\": checkpoint_callback.best_model_score.item() if checkpoint_callback.best_model_score else 0,\n",
    "        \"total_parameters\": sum(p.numel() for p in model.parameters()),\n",
    "        \"trainable_parameters\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "        \"temporal_encoding_enabled\": wandb.config.use_temporal_fusion,\n",
    "        \"temporal_dimensions\": wandb.config.temporal_dim\n",
    "    }\n",
    "    \n",
    "    wandb.log(final_metrics)\n",
    "    \n",
    "    # Create summary table\n",
    "    summary_data = []\n",
    "    for key, value in final_metrics.items():\n",
    "        summary_data.append([key, value])\n",
    "    \n",
    "    table = wandb.Table(data=summary_data, columns=[\"Metric\", \"Value\"])\n",
    "    wandb.log({\"final_metrics_table\": table})\n",
    "    \n",
    "    # Finish wandb run\n",
    "    wandb.finish()\n",
    "    \n",
    "    return model, trainer\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    # Start training\n",
    "    print(\"Starting Satellite ConvLSTM training with temporal encoding...\")\n",
    "    model, trainer = train_satellite_model()\n",
    "    print(\"Training completed!\")\n",
    "    print(\"View logs at: https://wandb.ai/\")\n",
    "    print(\"Best model saved in: checkpoints/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe4c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add wandb functionality\n",
    "#Int LST to Int LST\n",
    "#MSE RMSE MAE\n",
    "#Add time encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92530edb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
